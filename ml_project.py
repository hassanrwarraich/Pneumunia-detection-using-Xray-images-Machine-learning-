# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10XPDLx23yPOU7_VNCr8U6VjkUD27W6sU
"""

import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from pathlib import Path
import pandas as pd
import seaborn as sns
import cv2
from skimage.util import img_as_float32
from functools import partial
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Input, Flatten, SeparableConv2D, GlobalMaxPooling2D
from keras.layers.normalization import BatchNormalization

drive.mount('/content/drive')

data_dir = Path("/content/drive/MyDrive/train")

normal_dir = data_dir / "NORMAL"
pneumonia_dir = data_dir / "PNEUMONIA"

normal_cases = normal_dir.glob('*.jpeg')
pneumonia_cases = pneumonia_dir.glob('*.jpeg')


train_data = []

for img in normal_cases:
  train_data.append((img, 0))

for img in pneumonia_cases:
  train_data.append((img, 1))

train_data = pd.DataFrame(train_data, columns=["image", "label"],index=None)

train_data = train_data.sample(frac=1, random_state=42).reset_index(drop=True)

train_data.head()

# for i in range(10):
#   img = cv2.imread(str(train_data["image"][i]))
#   print(img.shape)
#   print(train_data["label"][i])

cases_count = train_data['label'].value_counts()
print(cases_count)

 
plt.figure(figsize=(10,8))
sns.barplot(x=cases_count.index, y= cases_count.values)
plt.title('Number of cases', fontsize=14)
plt.xlabel('Case type', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(range(len(cases_count.index)), ['Normal(0)', 'Pneumonia(1)'])
plt.show()

f, ax = plt.subplots(2,5, figsize=(30,10))

for i in range(10):
    img = cv2.imread(str(train_data["image"][i]))
    ax[i//5, i%5].imshow(img, cmap="gray")
    if train_data["label"][i] == 1:
        ax[i//5, i%5].set_title("Pneumonia")
    else:
        ax[i//5, i%5].set_title("Normal")
    ax[i//5, i%5].axis('off')
    ax[i//5, i%5].set_aspect('auto')
plt.show()

data_dir_test = Path("/content/drive/MyDrive/test")

def image_processing(data_dir):
  normal_dir = data_dir / "NORMAL"
  pneumonia_dir = data_dir / "PNEUMONIA"

  normal_cases = normal_dir.glob('*.jpeg')
  pneumonia_cases = pneumonia_dir.glob('*.jpeg')

  processed_data = []

  for img in normal_cases:
      
      img = cv2.imread(str(img))
      img = cv2.resize(img, (224,224))
      img = img_as_float32(img)
      processed_data.append([img, 0])

  for img in pneumonia_cases:
      
      img = cv2.imread(str(img))
      img = cv2.resize(img, (224,224))
      img = img_as_float32(img)
      processed_data.append([img,1])
      
  processed_data = np.array(processed_data)
  np.random.shuffle(processed_data)

  return processed_data

processed_data = image_processing(data_dir_test)



test_data = np.stack(processed_data[:, 0])
test_label = np.stack(processed_data[:, 1])

print(test_label.dtype)

np.save("test_data.npy",test_data)
np.save("test_label.npy",test_label)

DefaultConv2D = partial(SeparableConv2D, kernel_size=3, activation='relu', padding="same")

model = keras.models.Sequential([
  Input(shape=(224,224,3)),                               
  Conv2D(64, 3, activation='relu', padding='same'),
  Conv2D(64, 3, activation='relu', padding='same'),
  MaxPooling2D(pool_size=2),
  
  DefaultConv2D(filters=128),
  DefaultConv2D(filters=128),
  MaxPooling2D(pool_size=2),
  
  DefaultConv2D(filters=256),
  BatchNormalization(),
  DefaultConv2D(filters=256),
  BatchNormalization(),
  DefaultConv2D(filters=256),
  MaxPooling2D(pool_size=2),

  DefaultConv2D(filters=512),
  BatchNormalization(),
  DefaultConv2D(filters=512),
  BatchNormalization(),
  DefaultConv2D(filters=512),
  MaxPooling2D(pool_size=2),

  Flatten(),
  Dense(units=1024, activation='relu'),
  Dropout(0.5),
  Dense(units=512, activation='relu'),
  Dropout(0.5),
  Dense(units=1, activation='sigmoid')
])



# model.summary()

# setting the weights for first two convolutions and freezing them

pre_trained_weights = keras.applications.VGG16(weights='imagenet', include_top=False)

w, b = pre_trained_weights.layers[1].get_weights()
model.layers[1].set_weights = [w,b]
model.layers[1].trainable = False

w, b = pre_trained_weights.layers[2].get_weights()
model.layers[2].set_weights = [w,b]
model.layers[2].trainable = False

w, b = pre_trained_weights.layers[4].get_weights()
model.layers[4].set_weights = [w,b]
model.layers[4].trainable = False

w, b = pre_trained_weights.layers[5].get_weights()
model.layers[5].set_weights = [w,b]
model.layers[5].trainable = False

optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)
model.compile(loss="binary_crossentropy", optimizer=optimizer, metrics=["accuracy"])

#training the model with frozen first two set of convolution layers
history = model.fit(
    train_data,
    train_labels,
    batch_size=64,
    epochs=4,
    validation_data=(valid_data, valid_labels),
)

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()

# unfreezing the layers so that they can be trained
for layers in model.layers:
  layers.trainable = True

history = model.fit(
    train_data,
    train_labels,
    batch_size=64,
    epochs=25,
    validation_data=(valid_data, valid_labels),
)

pd.DataFrame(history.history).plot(figsize=(8, 5))
plt.grid(True)
plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]
plt.show()